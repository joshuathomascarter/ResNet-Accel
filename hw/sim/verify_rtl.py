#!/usr/bin/env python3
"""
verify_rtl.py — RTL Output Verification for ACCEL-BSR
======================================================

PURPOSE:
    Compares RTL simulation outputs against golden software model.
    Provides detailed error reporting with element-wise comparison,
    tolerance handling for INT8 quantization effects, and summary statistics.

USAGE:
    # After running Verilator simulation:
    cd hw/sim
    python3 verify_rtl.py [--golden golden_output.bin] [--rtl rtl_output.bin]

    # Or with custom tolerance:
    python3 verify_rtl.py --tolerance 1

INPUT FILES (generated by Verilator test):
    - golden_output.bin: INT32 golden model outputs (binary)
    - rtl_output.bin: INT32 RTL outputs (binary)
    - verify_metadata.json: Test configuration and performance data

OUTPUT:
    - Detailed comparison report
    - PASS/FAIL summary
    - Error distribution if failures occur

Author: ACCEL-BSR Team
"""

import os
import sys
import json
import argparse
import numpy as np
from pathlib import Path
from typing import Tuple, Dict, List, Optional


# =============================================================================
# Constants
# =============================================================================
DEFAULT_TOLERANCE = 0  # INT32 accumulation should be exact
MAX_ERRORS_TO_SHOW = 20  # Limit verbose output for large mismatches


# =============================================================================
# File Loading
# =============================================================================
def load_binary_int32(filepath: str) -> np.ndarray:
    """Load binary file as INT32 array."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Output file not found: {filepath}")
    
    data = np.fromfile(filepath, dtype=np.int32)
    return data


def load_metadata(filepath: str = "verify_metadata.json") -> Dict:
    """Load verification metadata JSON."""
    if os.path.exists(filepath):
        with open(filepath, 'r') as f:
            return json.load(f)
    return {}


# =============================================================================
# Comparison Logic
# =============================================================================
def compare_outputs(
    golden: np.ndarray,
    rtl: np.ndarray,
    tolerance: int = 0
) -> Tuple[bool, Dict]:
    """
    Compare RTL outputs against golden reference.

    TOLERANCE HANDLING:
        For INT32 accumulation, tolerance should be 0 (exact match).
        For INT8 quantized outputs, tolerance of 1-2 may be needed due to:
        - Rounding differences between fixed-point and floating-point
        - Order of operations in parallel reduction

    Args:
        golden: Golden reference outputs (INT32)
        rtl: RTL simulation outputs (INT32)
        tolerance: Maximum allowed absolute difference

    Returns:
        (pass, stats) where stats contains comparison details
    """
    # Handle size mismatch
    min_len = min(len(golden), len(rtl))
    max_len = max(len(golden), len(rtl))
    
    if len(golden) != len(rtl):
        print(f"⚠️  Size mismatch: golden={len(golden)}, rtl={len(rtl)}")
        print(f"    Comparing first {min_len} elements")
    
    # Compare common elements
    golden_cmp = golden[:min_len]
    rtl_cmp = rtl[:min_len]
    
    diff = np.abs(golden_cmp.astype(np.int64) - rtl_cmp.astype(np.int64))
    matches = diff <= tolerance
    
    num_match = np.sum(matches)
    num_mismatch = min_len - num_match
    match_rate = 100.0 * num_match / min_len if min_len > 0 else 0.0
    
    stats = {
        'golden_count': len(golden),
        'rtl_count': len(rtl),
        'compared_count': min_len,
        'num_match': int(num_match),
        'num_mismatch': int(num_mismatch),
        'match_rate': match_rate,
        'tolerance': tolerance,
        'max_diff': int(np.max(diff)) if min_len > 0 else 0,
        'mean_diff': float(np.mean(diff)) if min_len > 0 else 0.0,
        'mismatch_indices': np.where(~matches)[0].tolist() if num_mismatch > 0 else [],
    }
    
    # Pass if all match within tolerance
    passed = (num_mismatch == 0)
    
    return passed, stats


def print_comparison_report(
    golden: np.ndarray,
    rtl: np.ndarray,
    stats: Dict,
    metadata: Dict
) -> None:
    """Print detailed comparison report."""
    
    print("\n" + "="*70)
    print("RTL VERIFICATION REPORT")
    print("="*70)
    
    # Test info
    if metadata:
        print(f"\nLayer: {metadata.get('layer', 'unknown')}")
        print(f"Block size: {metadata.get('block_size', '?')}×{metadata.get('block_size', '?')}")
        print(f"Dimensions: M={metadata.get('M', '?')}, K={metadata.get('K', '?')}, N={metadata.get('N', '?')}")
        print(f"Blocks: {metadata.get('num_blocks', '?')}")
        print(f"Operations: {metadata.get('num_ops', '?')}")
        if 'perf_total_cycles' in metadata:
            print(f"Cycles: {metadata['perf_total_cycles']}")
    
    # Comparison summary
    print(f"\n--- Comparison Summary ---")
    print(f"Golden outputs:  {stats['golden_count']}")
    print(f"RTL outputs:     {stats['rtl_count']}")
    print(f"Compared:        {stats['compared_count']}")
    print(f"Tolerance:       {stats['tolerance']}")
    print(f"Matches:         {stats['num_match']} ({stats['match_rate']:.1f}%)")
    print(f"Mismatches:      {stats['num_mismatch']}")
    print(f"Max diff:        {stats['max_diff']}")
    print(f"Mean diff:       {stats['mean_diff']:.2f}")
    
    # Show first few values
    print(f"\n--- Output Sample (first 10) ---")
    min_len = min(len(golden), len(rtl), 10)
    print(f"{'Index':>6} {'Golden':>12} {'RTL':>12} {'Diff':>8} {'Status':>8}")
    print("-" * 50)
    for i in range(min_len):
        g = golden[i]
        r = rtl[i]
        d = abs(int(g) - int(r))
        status = "PASS" if d <= stats['tolerance'] else "FAIL"
        print(f"{i:>6} {g:>12} {r:>12} {d:>8} {status:>8}")
    
    # Show mismatches
    if stats['num_mismatch'] > 0:
        print(f"\n--- Mismatch Details (first {min(MAX_ERRORS_TO_SHOW, stats['num_mismatch'])}) ---")
        mismatch_idx = stats['mismatch_indices'][:MAX_ERRORS_TO_SHOW]
        for idx in mismatch_idx:
            if idx < len(golden) and idx < len(rtl):
                g = golden[idx]
                r = rtl[idx]
                d = int(g) - int(r)
                print(f"  [{idx:>4}] Golden={g:>10}, RTL={r:>10}, Diff={d:>+8}")
        
        if stats['num_mismatch'] > MAX_ERRORS_TO_SHOW:
            print(f"  ... and {stats['num_mismatch'] - MAX_ERRORS_TO_SHOW} more mismatches")


def print_result(passed: bool) -> None:
    """Print final PASS/FAIL result."""
    print("\n" + "="*70)
    if passed:
        print("║" + " "*28 + "✅ PASS" + " "*28 + "║")
    else:
        print("║" + " "*28 + "❌ FAIL" + " "*28 + "║")
    print("="*70)


# =============================================================================
# Golden Model (Python implementation for reference)
# =============================================================================
def golden_bsr_gemm_int8(
    activations: np.ndarray,
    bsr_dir: str,
    M: int, K: int, N: int = 1
) -> np.ndarray:
    """
    Python golden model for BSR GEMM.

    Computes: C = A @ B where B is in BSR format

    Args:
        activations: Input activations [K] (for N=1 GEMV case)
        bsr_dir: Directory containing BSR files (row_ptr.npy, col_idx.npy, weights.bsr)
        M: Output dimension
        K: Input dimension
        N: Batch size (usually 1)

    Returns:
        output: [M] INT32 accumulator values
    """
    import json

    # Load BSR data
    row_ptr = np.load(os.path.join(bsr_dir, 'row_ptr.npy'))
    col_idx = np.load(os.path.join(bsr_dir, 'col_idx.npy'))
    
    with open(os.path.join(bsr_dir, 'weights.meta.json'), 'r') as f:
        meta = json.load(f)
    
    block_h, block_w = meta['blocksize']
    num_blocks = meta['num_blocks']
    
    # Load weights
    weights_file = os.path.join(bsr_dir, 'weights.bsr')
    weights_flat = np.fromfile(weights_file, dtype=np.int8)
    
    # Reshape to [num_blocks, block_h, block_w]
    block_size = block_h * block_w
    expected_size = num_blocks * block_size
    if len(weights_flat) != expected_size:
        raise ValueError(f"Weight size mismatch: {len(weights_flat)} != {expected_size}")
    
    weights = weights_flat.reshape(num_blocks, block_h, block_w)
    
    # Compute BSR GEMM
    num_block_rows = len(row_ptr) - 1
    output = np.zeros(M, dtype=np.int32)
    
    for block_row in range(num_block_rows):
        row_start = row_ptr[block_row]
        row_end = row_ptr[block_row + 1]
        
        for b in range(row_start, row_end):
            block_col = col_idx[b]
            block_data = weights[b]  # [block_h, block_w]
            
            # Output rows this block affects
            out_row_start = block_row * block_h
            
            # Activation indices this block reads
            act_start = block_col * block_w
            
            for i in range(block_h):
                out_row = out_row_start + i
                if out_row >= M:
                    continue
                
                for j in range(block_w):
                    act_idx = act_start + j
                    if act_idx >= K:
                        continue
                    
                    w = int(block_data[i, j])
                    a = int(activations[act_idx])
                    output[out_row] += w * a
    
    return output


# =============================================================================
# Main Entry Point
# =============================================================================
def main():
    parser = argparse.ArgumentParser(
        description='Verify RTL outputs against golden model'
    )
    parser.add_argument(
        '--golden', type=str, default='golden_output.bin',
        help='Path to golden output binary file'
    )
    parser.add_argument(
        '--rtl', type=str, default='rtl_output.bin',
        help='Path to RTL output binary file'
    )
    parser.add_argument(
        '--metadata', type=str, default='verify_metadata.json',
        help='Path to verification metadata JSON'
    )
    parser.add_argument(
        '--tolerance', type=int, default=DEFAULT_TOLERANCE,
        help='Maximum allowed difference for match (default: 0)'
    )
    parser.add_argument(
        '--bsr-dir', type=str, default=None,
        help='BSR directory to regenerate golden (optional)'
    )
    
    args = parser.parse_args()
    
    # Change to script directory if files not found
    if not os.path.exists(args.golden):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        args.golden = os.path.join(script_dir, args.golden)
        args.rtl = os.path.join(script_dir, args.rtl)
        args.metadata = os.path.join(script_dir, args.metadata)
    
    # Load data
    try:
        golden = load_binary_int32(args.golden)
        rtl = load_binary_int32(args.rtl)
        metadata = load_metadata(args.metadata)
    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("\nRun the Verilator simulation first to generate output files:")
        print("  cd hw/sim && make mnist && ./obj_dir/Vmnist_bsr")
        return 1
    
    # Compare
    passed, stats = compare_outputs(golden, rtl, args.tolerance)
    
    # Report
    print_comparison_report(golden, rtl, stats, metadata)
    print_result(passed)
    
    return 0 if passed else 1


if __name__ == '__main__':
    sys.exit(main())
